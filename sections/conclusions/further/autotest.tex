\subsubsection{Automatic Testing and Verification}

Despite the size and coverage of the current test suite, it is fundamentally limited to what someone could write by hand. For a more comprehensive view of the SUTs' performance and functionality characteristics, much more test cases would be optimal, specifically those with different sizes, structures and complexities. Writing hundreds of thousands of tests by hand would quickly become infeasible, even with generators.

Csmith \cite{csmith} is a tool that can generate an unlimited number of valid C programs. This combined with GCC \cite{gcc} cross compiled for MIPS \cite{mips-gcc} can be used to generate an unlimited number of valid MIPS assembly/binary files. These tests can then be added to the test directory and consumed by the existing testing framework.

Currently, the test framework is not directly compatible with the output of GCC for a few reasons:

\begin{itemize}
    \item \textbf{Compiler Directives}
    
    The parser can be upgraded to consume the compiler directives either ignoring them or actioning them if applicable.

    \item \textbf{No Entry Point}
    
    The generated assembly does not have a compiled entry point and instead the \texttt{.ent} directive is used to indicate the main function as being an entry point. A simple post processor script can be used to auto generate entry point assembly at the beginning of the file that calls the main function appropriately.
\end{itemize}

The output of Csmith is not guaranteed to terminate \cite{csmith-paper}. This would cause the test runner to stall indefinitely whilst trying to measure the performance. To remedy this, a post-processing step can be added to the test generation process. Once a test has been completely generated by the previous steps, it can be invoked directly on the emulator with a timeout; if the timeout is exceeded then the test will be discarded from the suite and re-generated.

With this we would be able to greatly improve the coverage of the performance data. Unfortunately, automatic test generation alone is unable to improve the functional aspect of the testbench. Without an oracle, it is impossible to determine what results the tests should produce. If we guarantee that all generated tests are deterministic and free from undefined behaviour, we can exploit this property to create a derived oracle. Given this, a single test should produce the exact same results no matter what SUT it is run on, given that the SUT is correct.

This can be used for a form of automatic verification. All tests can be run on all SUTs, and the results can be cross compared. Any tests that do not exhibit consistent behaviour on all SUTs will be flagged for manual testing to detect the issue. This can help detect defects in the test themselves (such as containing undefined behaviour) or as a basic form of functionality testing for tests without any assertions defined; this can be useful for the output of Csmith.